{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install numpy\n!pip install scikit-learn\n!pip install scipy\n!pip install Theano",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Requirement already satisfied: numpy in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (1.13.3)\n\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: scikit-learn in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (0.19.0)\n\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: scipy in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (0.19.1)\nRequirement already satisfied: numpy>=1.8.2 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from scipy) (1.13.3)\n\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: Theano in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (0.7.0)\nRequirement already satisfied: numpy>=1.6.2 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from Theano) (1.13.3)\nRequirement already satisfied: scipy>=0.11 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from Theano) (0.19.1)\n\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\"\"\"\n    sizes contains the number of neurons in respective layers\n    Biases and weights are initialized randomly\n\"\"\"\nclass  Network(object):\n    def __init__(self, sizes):\n        self.num_layers = len(sizes)\n        self.sizes = sizes\n        # generated randomly\n        self.biases = [np.random.randn(y, 1) for y in size[1:]]\n        # generated randomly\n        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], size[1:])]\n    \n    def simoid(z):\n        return 1.0 / (1.0 + np.exp(-z))\n    \n    def feedforward(self, a):\n        \"\"\"Return the output of the network if 'a' is input \"\"\"\n        for b, w in zip(self.biases, self.weights):\n            a = sigmoid(np.dot(w, a) + b)\n        \n        return a\n    \n    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n        \"\"\"\n        Train network using mini-batch stochastic gradient descent. \n       \n        training_data: list of tuple  (x, y) representing the training inputs and the desired outputs. \n        \n        test_data: if provided then network will be evaluated against test data after each epoch.\n        \n        eta: learning rate\n        \"\"\"\n        \n        if test_data:\n            n_test = len(test_data)\n            n = len(training_data)\n            \n        for j in xrange(epochs):\n            # randomly shuffle training data\n            random_shuffle(training_data)\n            # partitions into mini_batches of approp size\n            mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)]\n            \n            # for each mini_batch we apply a single step of \n            # gradient descent. \n            for mini_batch in mini_batches:\n                # updates network weights and biases according to \n                # single iteration of gradient decent\n                self.update_mini_batch(mini_batch, eta)\n        \n        if test_data:\n            print(\"Epoch {0}: {1} / {2}\").format(j, self.evaluate(test_data), n_test)\n        else:\n            print(\"Epoch {0} complete\".format(j))\n            \n    def update_mini_batch(self, mini_batch, eta):\n        \"\"\"\n        Update networks weights and biases by applying gradient descent using\n        backpropagation to a single mini batch\n        \"\"\"\n        \n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        \n        for x, y in mini_batch:\n            # backpropagation algo, fast way of computing the \n            # gradient of the cost function. \n            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n            \n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw_dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n            \n        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n        self.biases = [b-(eta/len(mini_batch))*nw for b, nb in zip(self.biases, nabla_b)]",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create a netwrokd with 2 neurons in the first layer, 3 neurons \n# in the second layer, and 1 neuron in the final layer\nnet = Network([2, 3, 1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "$\n\\begin{align*}\n    a^{'} = \\sigma(wa +b)\n\\end{align*}\n$\n\n$a$ is a vector of activations of the second layer of neurons. To obtain $a^{\\prime}$ we multiply $a$ by the weight matrix $w$, and add the $b$ of biases.  "
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}